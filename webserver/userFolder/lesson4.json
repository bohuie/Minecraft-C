[
    {
        "instructions": "<h1>Emerald Collection Competition (K-nearest neighbors)</h1><br/> <h3>Hi welcome to the Emerald Collection Competition lesson.<br/>In this lesson you will use `K-nearest neighbors (KNN)` algorithm and your agent to to win Emerald!</h3> <br/><img src='../static/img/lesson4/Background.png' class='relative'/> </br>(this is a tutorial page please click NEXT to the next page)",
        "starterCode": "print(\"hi\")\r\n\r\n",
        "code": "print(\"hi\")\r\n\r\n",
        "answer": "Hello World!",
        "plotname": "",
        "haveplot": false,
        "nextLocked": false
    },
    {
        "instructions": "<h3>Background:</h3><p>Someone holds a machine learning competition in the village, and the winner could win a lot of emeralds. There are four zones in the village, only one of which is buried with emeralds, and each contestant has only one chance to dig for the treasure. The plants in each area are the key to determining the presence of emeralds. Here is the data provided by the organizers so that contestants can find out the relationship between the plants and the treasure. Let's use your robot to collect plants and predict the location of the treasure!</p> </br><p>4 start points in the competition </br></br>Treasure 1: [49, 4, -24]</br>Treasure 2: [36, 4, -26]</br>Treasure 3: [37, 4, -8]</br>Treasure 4: [50, 4, -7]</p> </br><p>If you never learn classification before, you might be wondering How ??? Let's learn some background first</p></br><p>(this is a tutorial page please click NEXT to the next page) </p>",
        "starterCode": "print(\"Hello World!\")",
        "code": "print(\"Hello World!\")",
        "answer": "Hello World from part 2!",
        "plotname": "",
        "haveplot": false,
        "nextLocked": false
    },
    {
        "instructions": "<h3>Supervised Learning and Unsupervised learning</h3></br><h4>Supervised Learning</h4></br><p>In supervised learning, we use a set of labeled data to train our model. For example, in a classification task, if we want our model to be able to distinguish between dogs and cats in a set of photos, we can use a set of images and label them with cat or dog. After the training use this data, our model will be able to identify the new image and label it as a cat or a dog.</p></br><h4>Unsupervised Learning</h4></br><p>In unsupervised learning, we use unlabeled data to train the model. The model will analyze the data and find patterns in it. For example, if we train a model by a set of pictures of cat and dog, but without any labels, the model could divide photos into two categories and if we provide a new image of cat, the model will put the picture in the same category as the cat. </p></br><p>(this is a tutorial page please click NEXT to the next page)</p>",
        "starterCode": " ",
        "code": "",
        "answer": "",
        "plotname": "",
        "haveplot": false,
        "nextLocked": false
    },
    {
        "instructions": "<h3>K-nearest neighbors (KNN)</h3></br><p>In this lesson we are going to learn a supervised learning algorithm called K-nearst neighbors. The idea of this algorithm is very simple, given an input point, find the nearest K neighbors to this point, and predict the type of the input point based on the type of the neighbor. For example, the red dot in the figure below is the input value, and we find the 3 nearest neighbors.</p></br><img src='../static/img/lesson4/KNN.jpg' class='relative'/></br><p>(this is a tutorial page please click NEXT to the next page)</p>",
        "starterCode": " ",
        "code": "",
        "answer": "",
        "plotname": "",
        "haveplot": false,
        "nextLocked": false
    },
    {
        "instructions": "<h3>Euclidean Distance</h3></br><p>You might be wondering how do we calculate the distance between two data points, and the answer is Euclidean distance. Here is the Formula:</p> </br><img src='../static/img/lesson4/Euclidean.png' class='relative'/> </br><p>a and b in the formula are two points (vector) on the vector space, and we can easily visualize their distance in a two-dimensional coordinate system (see graph) </p> </br><img src='../static/img/lesson4/Distance graph.jpg' class='relative'/> </br><p></p> </br><p>When vectors have more dimensions, it's hard to visualize on a graph. But the distance formula also works for higher dimensional vectors.So for implement KNN, the first step is calculate distance for all vectors.</p> </br><p>Now please use the Euclidean distance to find the distance between <span style='color: rgb(235, 107, 86);'>point[2,5]</span> and <span style='color: rgb(235, 107, 86);'>[2,8]</span></p></br><p>(this is a coding page please run the code before go to next page)</p>",
        "starterCode": "# Euclidean distance Python implementation based on the formula\r\ndef Euclidean_dis(mine1,mine2):\r\n    diff=0\r\n    for x in range(len(mine1)):\r\n        diff+=(mine1[x]-mine2[x])**2\r\n    return diff**0.5\r\n\r\n",
        "code": "# Euclidean distance Python implementation based on the formula\r\ndef Euclidean_dis(mine1,mine2):\r\n    diff=0\r\n    for x in range(len(mine1)):\r\n        diff+=(mine1[x]-mine2[x])**2\r\n    return diff**0.5\r\n\r\n",
        "answer": "",
        "plotname": "",
        "haveplot": false,
        "nextLocked": false
    },
    {
        "instructions": "<h3>Normalization</h3></br><p>The next step is normalize our data, but what is normalization and why?</p> </br><p>Consider this example: If we want to determine the type of a mob in Minecraft based on some of attributes, we can consider the maximum health and the movement speed of the mob. However, the movement speed of the mob is normally less than 2 points, but its maximum health can exceed 1,000 points. There is a problem; when we calculate the distance between each vector, all attributes are treated equally. Therefore, if a mob is 1 point faster than the other, a large difference on the the movement speed range, we also consider that a 1 point speed difference has the same importance as a 1 points difference of the maximum health, a small difference on the maximum health range. This is an unequal comparison. So we need to normalize the data before we can calculate their distance.</p> </br><p>(this is a tutorial page please click NEXT to the next page)</p>",
        "starterCode": " ",
        "code": "",
        "answer": "",
        "plotname": "",
        "haveplot": false,
        "nextLocked": false
    },
    {
        "instructions": "<h3>Min-max normalization</h3></br><p>Min-max normalization is a commonly used normalization method, which can help us convert all the data in to the range of 0 to 1 (the largest number in the data set becomes 1, the smallest becomes 0 and the rest becomes a decimal between 1 and 0). This approach will help us make all data equally important and this is the formula:</p> <br/><img src='../static/img/lesson4/Min_max.png' class='relative'/></br><p>Let's implement Min-max normalization by Python!</p> </br><p>(this is a coding page please run the code before go to next page)</p>",
        "starterCode": "# Min-max normalization Python implementation based on formula\r\ndef norm(x,lis):\r\n    mini=min(lis)\r\n    maxi=max(lis)\r\n    normalize=(x-mini)/(maxi-mini)\r\n    return normalize\r\n\r\nlis=[1,8,12,29,34,56] #test case\r\n\r\nprint(norm(1,lis)) \r\n#1 is the smallest number in the list so the output is 0\r\nprint(norm(56,lis))\r\n#56 is the largest number in the list so the output is 1\r\nprint(norm(34,lis))\r\n#the output of 34 is a number between 1 and 0",
        "code": "# Min-max normalization Python implementation based on formula\r\ndef norm(x,lis):\r\n    mini=min(lis)\r\n    maxi=max(lis)\r\n    normalize=(x-mini)/(maxi-mini)\r\n    return normalize\r\n\r\nlis=[1,8,12,29,34,56] #test case\r\n\r\nprint(norm(1,lis)) \r\n#1 is the smallest number in the list so the output is 0\r\nprint(norm(56,lis))\r\n#56 is the largest number in the list so the output is 1\r\nprint(norm(34,lis))\r\n#the output of 34 is a number between 1 and 0",
        "answer": "",
        "plotname": "",
        "haveplot": false,
        "nextLocked": false
    },
    {
        "instructions": "<h3>KNN implementation</h3></br><p>Now we have the normalization function, let's calculate the distance between the input point and other points in the dataset.</p></br><p>(this is a coding page please run the code before go to next page)</p>",
        "starterCode": "import pandas as pd\r\n\r\n\r\ntrain_data=pd.read_csv(\"./data/LessonCla_train_data.csv\")\r\nval_data=pd.read_csv(\"./data/LessonCla_val_data.csv\")\r\n#import the data from csv file\r\n\r\n#print(train_data)\r\n#you can uncomment this line to see the data\r\n\r\nt_daisy=train_data['oxeye_daisy']\r\nt_allium=train_data['allium']\r\nt_orchid=train_data['blue_orchid']\r\nt_tulip=train_data['red_tulip']\r\nt_label=train_data['label']\r\n#assign the data to variables\r\n\r\ndef Euclidean_dis(mine1,mine2):\r\n    diff=0\r\n    for x in range(len(mine1)):\r\n        diff+=(mine1[x]-mine2[x])**2\r\n    return diff**0.5\r\n#Calculate Euclidean distance\r\n\r\ndef norm(x,lis):\r\n    mini=min(lis)\r\n    maxi=max(lis)\r\n    normalize=(x-mini)/(maxi-mini)\r\n    return normalize\r\n#calculate the normalized value for a single value\r\n\r\ndef normlist(lis):\r\n    normlist=[]\r\n    for x in lis:\r\n        normlist.append(norm(x,lis))\r\n    return normlist\r\n#calculate the normalized value for a list\r\n\r\ndef normsample(x):\r\n    x1=norm(x[0],t_daisy)\r\n    x2=norm(x[1],t_allium)\r\n    x3=norm(x[2],t_orchid)\r\n    x4=norm(x[3],t_tulip)\r\n    lis=[x1,x2,x3,x4]\r\n    return lis\r\n# because each sample is the list that contain 4 attributes\r\n#so we should compute the normalized value separately\r\n\r\nnt_daisy=normlist(t_daisy)\r\nnt_allium=normlist(t_allium)\r\nnt_orchid=normlist(t_orchid)\r\nnt_tulip=normlist(t_tulip)\r\n\r\ntrain_dataset=list(zip(nt_daisy,nt_allium,nt_orchid,nt_tulip))\r\n#zip 4 variables as a list\r\n\r\ncount=[]\r\nsample=[3,6,1,7]\r\nfor x in range(len(train_dataset)):\r\n        count.append([Euclidean_dis(normsample(sample),train_dataset[x]),t_label[x]])\r\nprint(count[0:3])   \r\n#print top 3 values",
        "code": "import pandas as pd\r\n\r\n\r\ntrain_data=pd.read_csv(\"./data/LessonCla_train_data.csv\")\r\nval_data=pd.read_csv(\"./data/LessonCla_val_data.csv\")\r\n#import the data from csv file\r\n\r\n#print(train_data)\r\n#you can uncomment this line to see the data\r\n\r\nt_daisy=train_data['oxeye_daisy']\r\nt_allium=train_data['allium']\r\nt_orchid=train_data['blue_orchid']\r\nt_tulip=train_data['red_tulip']\r\nt_label=train_data['label']\r\n#assign the data to variables\r\n\r\ndef Euclidean_dis(mine1,mine2):\r\n    diff=0\r\n    for x in range(len(mine1)):\r\n        diff+=(mine1[x]-mine2[x])**2\r\n    return diff**0.5\r\n#Calculate Euclidean distance\r\n\r\ndef norm(x,lis):\r\n    mini=min(lis)\r\n    maxi=max(lis)\r\n    normalize=(x-mini)/(maxi-mini)\r\n    return normalize\r\n#calculate the normalized value for a single value\r\n\r\ndef normlist(lis):\r\n    normlist=[]\r\n    for x in lis:\r\n        normlist.append(norm(x,lis))\r\n    return normlist\r\n#calculate the normalized value for a list\r\n\r\ndef normsample(x):\r\n    x1=norm(x[0],t_daisy)\r\n    x2=norm(x[1],t_allium)\r\n    x3=norm(x[2],t_orchid)\r\n    x4=norm(x[3],t_tulip)\r\n    lis=[x1,x2,x3,x4]\r\n    return lis\r\n# because each sample is the list that contain 4 attributes\r\n#so we should compute the normalized value separately\r\n\r\nnt_daisy=normlist(t_daisy)\r\nnt_allium=normlist(t_allium)\r\nnt_orchid=normlist(t_orchid)\r\nnt_tulip=normlist(t_tulip)\r\n\r\ntrain_dataset=list(zip(nt_daisy,nt_allium,nt_orchid,nt_tulip))\r\n#zip 4 variables as a list\r\n\r\ncount=[]\r\nsample=[3,6,1,7]\r\nfor x in range(len(train_dataset)):\r\n        count.append([Euclidean_dis(normsample(sample),train_dataset[x]),t_label[x]])\r\nprint(count[0:3])   \r\n#print top 3 values",
        "answer": "",
        "plotname": "",
        "haveplot": false,
        "nextLocked": false
    },
    {
        "instructions": "<h3>Make a prediction</h3></br><p>Now we have the distance between all points, let's find top k neighbors and make a prediction</p></br><p>(this is a coding page please run the code before go to next page)</p>",
        "starterCode": "import pandas as pd\r\n\r\n\r\ntrain_data=pd.read_csv(\"./data/LessonCla_train_data.csv\")\r\nval_data=pd.read_csv(\"./data/LessonCla_val_data.csv\")\r\n#import the data from csv file\r\n\r\n#print(train_data)\r\n#you can uncomment this line to see the data\r\n\r\n\r\nt_daisy=train_data['oxeye_daisy']\r\nt_allium=train_data['allium']\r\nt_orchid=train_data['blue_orchid']\r\nt_tulip=train_data['red_tulip']\r\nt_label=train_data['label']\r\n#assign the data to variables\r\n\r\ndef Euclidean_dis(mine1,mine2):\r\n    diff=0\r\n    for x in range(len(mine1)):\r\n        diff+=(mine1[x]-mine2[x])**2\r\n    return diff**0.5\r\n#Calculate Euclidean distance\r\n\r\ndef norm(x,lis):\r\n    mini=min(lis)\r\n    maxi=max(lis)\r\n    normalize=(x-mini)/(maxi-mini)\r\n    return normalize\r\n#calculate the normalized value for a single value\r\n\r\ndef normlist(lis):\r\n    normlist=[]\r\n    for x in lis:\r\n        normlist.append(norm(x,lis))\r\n    return normlist\r\n#calculate the normalized value for a list\r\n\r\ndef normsample(x):\r\n    x1=norm(x[0],t_daisy)\r\n    x2=norm(x[1],t_allium)\r\n    x3=norm(x[2],t_orchid)\r\n    x4=norm(x[3],t_tulip)\r\n    lis=[x1,x2,x3,x4]\r\n    return lis\r\n# because each sample is the list that contain 4 attributes\r\n#so we should compute the normalized value separately\r\n\r\nnt_daisy=normlist(t_daisy)\r\nnt_allium=normlist(t_allium)\r\nnt_orchid=normlist(t_orchid)\r\nnt_tulip=normlist(t_tulip)\r\n\r\ntrain_dataset=list(zip(nt_daisy,nt_allium,nt_orchid,nt_tulip))\r\n#zip 4 variables as a list\r\n  \r\n\r\n# Top K neighbors\r\ndef KNN(sample,t_dataset,k,t_label):\r\n    count=[]\r\n    #normsample=normsample(sample)\r\n    for x in range(len(t_dataset)):\r\n        count.append([Euclidean_dis(normsample(sample),t_dataset[x]),t_label[x]])\r\n        \r\n    count.sort() #sort the list and take top k values\r\n    kn=count[:k]\r\n    predict=0\r\n    for x in kn:\r\n        predict+=x[1]\r\n\r\n    if predict/k>0.5:\r\n        return 1\r\n    else:\r\n        return 0\r\n\t#determine the properties of the sample according to the \r\n\t#properties of the neighbors.\r\n\t\t\r\nprint(KNN([3,6,1,9],train_dataset,3,t_label))",
        "code": "import pandas as pd\r\n\r\n\r\ntrain_data=pd.read_csv(\"./data/LessonCla_train_data.csv\")\r\nval_data=pd.read_csv(\"./data/LessonCla_val_data.csv\")\r\n#import the data from csv file\r\n\r\n#print(train_data)\r\n#you can uncomment this line to see the data\r\n\r\n\r\nt_daisy=train_data['oxeye_daisy']\r\nt_allium=train_data['allium']\r\nt_orchid=train_data['blue_orchid']\r\nt_tulip=train_data['red_tulip']\r\nt_label=train_data['label']\r\n#assign the data to variables\r\n\r\ndef Euclidean_dis(mine1,mine2):\r\n    diff=0\r\n    for x in range(len(mine1)):\r\n        diff+=(mine1[x]-mine2[x])**2\r\n    return diff**0.5\r\n#Calculate Euclidean distance\r\n\r\ndef norm(x,lis):\r\n    mini=min(lis)\r\n    maxi=max(lis)\r\n    normalize=(x-mini)/(maxi-mini)\r\n    return normalize\r\n#calculate the normalized value for a single value\r\n\r\ndef normlist(lis):\r\n    normlist=[]\r\n    for x in lis:\r\n        normlist.append(norm(x,lis))\r\n    return normlist\r\n#calculate the normalized value for a list\r\n\r\ndef normsample(x):\r\n    x1=norm(x[0],t_daisy)\r\n    x2=norm(x[1],t_allium)\r\n    x3=norm(x[2],t_orchid)\r\n    x4=norm(x[3],t_tulip)\r\n    lis=[x1,x2,x3,x4]\r\n    return lis\r\n# because each sample is the list that contain 4 attributes\r\n#so we should compute the normalized value separately\r\n\r\nnt_daisy=normlist(t_daisy)\r\nnt_allium=normlist(t_allium)\r\nnt_orchid=normlist(t_orchid)\r\nnt_tulip=normlist(t_tulip)\r\n\r\ntrain_dataset=list(zip(nt_daisy,nt_allium,nt_orchid,nt_tulip))\r\n#zip 4 variables as a list\r\n  \r\n\r\n# Top K neighbors\r\ndef KNN(sample,t_dataset,k,t_label):\r\n    count=[]\r\n    #normsample=normsample(sample)\r\n    for x in range(len(t_dataset)):\r\n        count.append([Euclidean_dis(normsample(sample),t_dataset[x]),t_label[x]])\r\n        \r\n    count.sort() #sort the list and take top k values\r\n    kn=count[:k]\r\n    predict=0\r\n    for x in kn:\r\n        predict+=x[1]\r\n\r\n    if predict/k>0.5:\r\n        return 1\r\n    else:\r\n        return 0\r\n\t#determine the properties of the sample according to the \r\n\t#properties of the neighbors.\r\n\t\t\r\nprint(KNN([3,6,1,9],train_dataset,3,t_label))",
        "answer": "",
        "plotname": "",
        "haveplot": false,
        "nextLocked": false
    },
    {
        "instructions": "<h3>Verify the accuracy of your model</h3></br><p>Generally, we will divide our data into two groups in a ratio of <span style='color: rgb(235, 107, 86);'>1:9 or 2:8</span> as the training set and validation set. We will use training set to train our ML model and use validation set to test the performance of the model. Now let's use the validation set that we provided to evaluate the classification model.</p></br><p>(this is a coding page please run the code before go to next page)</p>",
        "starterCode": "import pandas as pd\r\n\r\n\r\ntrain_data=pd.read_csv(\"./data/LessonCla_train_data.csv\")\r\nval_data=pd.read_csv(\"./data/LessonCla_val_data.csv\")\r\n#import the data from csv file\r\n\r\n#print(train_data)\r\n#you can uncomment this line to see the data\r\n\r\n\r\nt_daisy=train_data['oxeye_daisy']\r\nt_allium=train_data['allium']\r\nt_orchid=train_data['blue_orchid']\r\nt_tulip=train_data['red_tulip']\r\nt_label=train_data['label']\r\n#assign the data to variables\r\n\r\ndef Euclidean_dis(mine1,mine2):\r\n    diff=0\r\n    for x in range(len(mine1)):\r\n        diff+=(mine1[x]-mine2[x])**2\r\n    return diff**0.5\r\n#Calculate Euclidean distance\r\n\r\ndef norm(x,lis):\r\n    mini=min(lis)\r\n    maxi=max(lis)\r\n    normalize=(x-mini)/(maxi-mini)\r\n    return normalize\r\n#calculate the normalized value for a single value\r\n\r\ndef normlist(lis):\r\n    normlist=[]\r\n    for x in lis:\r\n        normlist.append(norm(x,lis))\r\n    return normlist\r\n#calculate the normalized value for a list\r\n\r\ndef normsample(x):\r\n    x1=norm(x[0],t_daisy)\r\n    x2=norm(x[1],t_allium)\r\n    x3=norm(x[2],t_orchid)\r\n    x4=norm(x[3],t_tulip)\r\n    lis=[x1,x2,x3,x4]\r\n    return lis\r\n# because each sample is the list that contain 4 attributes\r\n#so we should compute the normalized value separately\r\n\r\nnt_daisy=normlist(t_daisy)\r\nnt_allium=normlist(t_allium)\r\nnt_orchid=normlist(t_orchid)\r\nnt_tulip=normlist(t_tulip)\r\n\r\ntrain_dataset=list(zip(nt_daisy,nt_allium,nt_orchid,nt_tulip))\r\n#zip 4 variables as a list\r\n  \r\n\r\n# Top K neighbors\r\ndef KNN(sample,t_dataset,k,t_label):\r\n    count=[]\r\n    #normsample=normsample(sample)\r\n    for x in range(len(t_dataset)):\r\n        count.append([Euclidean_dis(normsample(sample),t_dataset[x]),t_label[x]])\r\n        \r\n    count.sort() #sort the list and take top k values\r\n    kn=count[:k]\r\n    predict=0\r\n    for x in kn:\r\n        predict+=x[1]\r\n\r\n    if predict/k>0.5:\r\n        return 1\r\n    else:\r\n        return 0\r\n\t#determine the properties of the sample according to the \r\n\t#properties of the neighbors.\r\n\t\t\r\n\r\n\t\t\r\n# validation\r\nv_daisy=val_data['oxeye_daisy']\r\nv_allium=val_data['allium']\r\nv_orchid=val_data['blue_orchid']\r\nv_tulip=val_data['red_tulip']\r\nv_label=val_data['label']\r\n#make validation set\r\n\r\nval_dataset=list(zip(v_daisy,v_allium,v_orchid,v_tulip))\r\n\r\nval_count=0\r\nfor x in range (len(val_dataset)):\r\n    if KNN(val_dataset[x],train_dataset,3,t_label)==v_label[x]:\r\n        val_count+=1\r\n#compute the accuracy of the model\r\n        \r\nprint(val_count/len(v_label))",
        "code": "import pandas as pd\r\n\r\n\r\ntrain_data=pd.read_csv(\"./data/LessonCla_train_data.csv\")\r\nval_data=pd.read_csv(\"./data/LessonCla_val_data.csv\")\r\n#import the data from csv file\r\n\r\n#print(train_data)\r\n#you can uncomment this line to see the data\r\n\r\n\r\nt_daisy=train_data['oxeye_daisy']\r\nt_allium=train_data['allium']\r\nt_orchid=train_data['blue_orchid']\r\nt_tulip=train_data['red_tulip']\r\nt_label=train_data['label']\r\n#assign the data to variables\r\n\r\ndef Euclidean_dis(mine1,mine2):\r\n    diff=0\r\n    for x in range(len(mine1)):\r\n        diff+=(mine1[x]-mine2[x])**2\r\n    return diff**0.5\r\n#Calculate Euclidean distance\r\n\r\ndef norm(x,lis):\r\n    mini=min(lis)\r\n    maxi=max(lis)\r\n    normalize=(x-mini)/(maxi-mini)\r\n    return normalize\r\n#calculate the normalized value for a single value\r\n\r\ndef normlist(lis):\r\n    normlist=[]\r\n    for x in lis:\r\n        normlist.append(norm(x,lis))\r\n    return normlist\r\n#calculate the normalized value for a list\r\n\r\ndef normsample(x):\r\n    x1=norm(x[0],t_daisy)\r\n    x2=norm(x[1],t_allium)\r\n    x3=norm(x[2],t_orchid)\r\n    x4=norm(x[3],t_tulip)\r\n    lis=[x1,x2,x3,x4]\r\n    return lis\r\n# because each sample is the list that contain 4 attributes\r\n#so we should compute the normalized value separately\r\n\r\nnt_daisy=normlist(t_daisy)\r\nnt_allium=normlist(t_allium)\r\nnt_orchid=normlist(t_orchid)\r\nnt_tulip=normlist(t_tulip)\r\n\r\ntrain_dataset=list(zip(nt_daisy,nt_allium,nt_orchid,nt_tulip))\r\n#zip 4 variables as a list\r\n  \r\n\r\n# Top K neighbors\r\ndef KNN(sample,t_dataset,k,t_label):\r\n    count=[]\r\n    #normsample=normsample(sample)\r\n    for x in range(len(t_dataset)):\r\n        count.append([Euclidean_dis(normsample(sample),t_dataset[x]),t_label[x]])\r\n        \r\n    count.sort() #sort the list and take top k values\r\n    kn=count[:k]\r\n    predict=0\r\n    for x in kn:\r\n        predict+=x[1]\r\n\r\n    if predict/k>0.5:\r\n        return 1\r\n    else:\r\n        return 0\r\n\t#determine the properties of the sample according to the \r\n\t#properties of the neighbors.\r\n\t\t\r\n\r\n\t\t\r\n# validation\r\nv_daisy=val_data['oxeye_daisy']\r\nv_allium=val_data['allium']\r\nv_orchid=val_data['blue_orchid']\r\nv_tulip=val_data['red_tulip']\r\nv_label=val_data['label']\r\n#make validation set\r\n\r\nval_dataset=list(zip(v_daisy,v_allium,v_orchid,v_tulip))\r\n\r\nval_count=0\r\nfor x in range (len(val_dataset)):\r\n    if KNN(val_dataset[x],train_dataset,3,t_label)==v_label[x]:\r\n        val_count+=1\r\n#compute the accuracy of the model\r\n        \r\nprint(val_count/len(v_label))",
        "answer": "",
        "plotname": "",
        "haveplot": false,
        "nextLocked": false
    },
    {
        "instructions": "<h3>Program your robot</h3></br><p>Now we have created a KNN classification model, let's program your robot to collect the plants and predict which area has emeralds. (The answer is on the next page)</p></br><p>You can find the location of each area here:</br>Treasure 1: [49, 4, -24]</br>Treasure 2: [36, 4, -26]</br>Treasure 3: [37, 4, -8]</br>Treasure 4: [50, 4, -7]</p></br><p>(this is a coding page please run the code before go to next page)</p>",
        "starterCode": "\r\n#agent.give(item, quantity, slot number)\r\n#\tPuts a block or item in the agent's inventory slot\r\n\r\n\r\n\r\n#agent.moveForward()\r\n#agent.moveBack()\r\n#agent.moveLeft()\r\n#agent.moveRight()\r\n#agent.moveUp()\r\n#agent.moveDown()\r\n#\tAttempts to move the agent in the given direction\r\n\r\n\r\n\r\n\r\n#agent.turnLeft()\r\n#agent.turnRight()\r\n#\tTurns the agent in the given direction\r\n\r\n\r\n\r\n#agent.tp()\r\n#agent.tpto(location)\r\n#\tTeleports the agent to a location if specified, or else to the player\r\n\r\n\r\n\r\n#agent.place(slot number, direction)\r\n#\tPlaces a block or uses an item from the agent's inventory",
        "code": "\r\n#agent.give(item, quantity, slot number)\r\n#\tPuts a block or item in the agent's inventory slot\r\n\r\n\r\n\r\n#agent.moveForward()\r\n#agent.moveBack()\r\n#agent.moveLeft()\r\n#agent.moveRight()\r\n#agent.moveUp()\r\n#agent.moveDown()\r\n#\tAttempts to move the agent in the given direction\r\n\r\n\r\n\r\n\r\n#agent.turnLeft()\r\n#agent.turnRight()\r\n#\tTurns the agent in the given direction\r\n\r\n\r\n\r\n#agent.tp()\r\n#agent.tpto(location)\r\n#\tTeleports the agent to a location if specified, or else to the player\r\n\r\n\r\n\r\n#agent.place(slot number, direction)\r\n#\tPlaces a block or uses an item from the agent's inventory",
        "answer": "",
        "plotname": "",
        "haveplot": false,
        "nextLocked": false
    },
    {
        "instructions": "<h3>Program your robot answer</h3></br><p>answer page</p></br><p>You can find the location of each area here:</br>Treasure 1: [49, 4, -24]</br>Treasure 2: [36, 4, -26]</br>Treasure 3: [37, 4, -8]</br>Treasure 4: [50, 4, -7]</p></br><p>(this is a coding page please run the code before go to next page)</p>",
        "starterCode": "#program your robot\r\ndef turnback():\r\n    agent.turnLeft()\r\n    agent.turnLeft()\r\n\r\ndef breakMoveCollect():\r\n    agent.destroy('forward')\r\n    agent.moveForward()\r\n    agent.collect('all')\r\n\r\n#agent.moveForward()\r\n\r\n\r\ndef collect(x,y):\r\n    for i in range(x):\r\n        for _ in range(y):\r\n            breakMoveCollect()\r\n\r\n        if(i < x):\r\n            if(i % 2 == 0):\r\n                agent.turnRight()\r\n                breakMoveCollect()\r\n                agent.turnRight()\r\n            else:\r\n                agent.turnLeft()\r\n                breakMoveCollect()\r\n                agent.turnLeft()\r\n    agent.moveForward()\r\n\r\n#agent.tpto(49, 4, -24) #you can use this line to teleport your agent\r\n#turnback()\r\ncollect(5,5)",
        "code": "#program your robot\r\ndef turnback():\r\n    agent.turnLeft()\r\n    agent.turnLeft()\r\n\r\ndef breakMoveCollect():\r\n    agent.destroy('forward')\r\n    agent.moveForward()\r\n    agent.collect('all')\r\n\r\n#agent.moveForward()\r\n\r\n\r\ndef collect(x,y):\r\n    for i in range(x):\r\n        for _ in range(y):\r\n            breakMoveCollect()\r\n\r\n        if(i < x):\r\n            if(i % 2 == 0):\r\n                agent.turnRight()\r\n                breakMoveCollect()\r\n                agent.turnRight()\r\n            else:\r\n                agent.turnLeft()\r\n                breakMoveCollect()\r\n                agent.turnLeft()\r\n    agent.moveForward()\r\n\r\n#agent.tpto(49, 4, -24) #you can use this line to teleport your agent\r\n#turnback()\r\ncollect(5,5)",
        "answer": "",
        "plotname": "",
        "haveplot": false,
        "nextLocked": false
    },
    {
        "instructions": "<h3>Find Emerald</h3></br><p>Now you have the data for each region, let's use the KNN algorithm to find the emerald!</p></br><p>(this is a coding page please run the code before go to next page)</p>",
        "starterCode": "import pandas as pd\r\n\r\n\r\ntrain_data=pd.read_csv(\"./data/LessonCla_train_data.csv\")\r\nval_data=pd.read_csv(\"./data/LessonCla_val_data.csv\")\r\n#import the data from csv file\r\n\r\n#print(train_data)\r\n#you can uncomment this line to see the data\r\n\r\n\r\nt_daisy=train_data['oxeye_daisy']\r\nt_allium=train_data['allium']\r\nt_orchid=train_data['blue_orchid']\r\nt_tulip=train_data['red_tulip']\r\nt_label=train_data['label']\r\n#assign the data to variables\r\n\r\ndef Euclidean_dis(mine1,mine2):\r\n    diff=0\r\n    for x in range(len(mine1)):\r\n        diff+=(mine1[x]-mine2[x])**2\r\n    return diff**0.5\r\n#Calculate Euclidean distance\r\n\r\ndef norm(x,lis):\r\n    mini=min(lis)\r\n    maxi=max(lis)\r\n    normalize=(x-mini)/(maxi-mini)\r\n    return normalize\r\n#calculate the normalized value for a single value\r\n\r\ndef normlist(lis):\r\n    normlist=[]\r\n    for x in lis:\r\n        normlist.append(norm(x,lis))\r\n    return normlist\r\n#calculate the normalized value for a list\r\n\r\ndef normsample(x):\r\n    x1=norm(x[0],t_daisy)\r\n    x2=norm(x[1],t_allium)\r\n    x3=norm(x[2],t_orchid)\r\n    x4=norm(x[3],t_tulip)\r\n    lis=[x1,x2,x3,x4]\r\n    return lis\r\n# because each sample is the list that contain 4 attributes\r\n#so we should compute the normalized value separately\r\n\r\nnt_daisy=normlist(t_daisy)\r\nnt_allium=normlist(t_allium)\r\nnt_orchid=normlist(t_orchid)\r\nnt_tulip=normlist(t_tulip)\r\n\r\ntrain_dataset=list(zip(nt_daisy,nt_allium,nt_orchid,nt_tulip))\r\n#zip 4 variables as a list\r\n  \r\n\r\n# Top K neighbors\r\ndef KNN(sample,t_dataset,k,t_label):\r\n    count=[]\r\n    #normsample=normsample(sample)\r\n    for x in range(len(t_dataset)):\r\n        count.append([Euclidean_dis(normsample(sample),t_dataset[x]),t_label[x]])\r\n        \r\n    count.sort() #sort the list and take top k values\r\n    kn=count[:k]\r\n    predict=0\r\n    for x in kn:\r\n        predict+=x[1]\r\n\r\n    if predict/k>0.5:\r\n        return 1\r\n    else:\r\n        return 0\r\n\t#determine the properties of the sample according to the \r\n\t#properties of the neighbors.\r\n\t\t\r\n\t\t\r\n#print(KNN([3,6,1,9],train_dataset,3,t_label))",
        "code": "import pandas as pd\r\n\r\n\r\ntrain_data=pd.read_csv(\"./data/LessonCla_train_data.csv\")\r\nval_data=pd.read_csv(\"./data/LessonCla_val_data.csv\")\r\n#import the data from csv file\r\n\r\n#print(train_data)\r\n#you can uncomment this line to see the data\r\n\r\n\r\nt_daisy=train_data['oxeye_daisy']\r\nt_allium=train_data['allium']\r\nt_orchid=train_data['blue_orchid']\r\nt_tulip=train_data['red_tulip']\r\nt_label=train_data['label']\r\n#assign the data to variables\r\n\r\ndef Euclidean_dis(mine1,mine2):\r\n    diff=0\r\n    for x in range(len(mine1)):\r\n        diff+=(mine1[x]-mine2[x])**2\r\n    return diff**0.5\r\n#Calculate Euclidean distance\r\n\r\ndef norm(x,lis):\r\n    mini=min(lis)\r\n    maxi=max(lis)\r\n    normalize=(x-mini)/(maxi-mini)\r\n    return normalize\r\n#calculate the normalized value for a single value\r\n\r\ndef normlist(lis):\r\n    normlist=[]\r\n    for x in lis:\r\n        normlist.append(norm(x,lis))\r\n    return normlist\r\n#calculate the normalized value for a list\r\n\r\ndef normsample(x):\r\n    x1=norm(x[0],t_daisy)\r\n    x2=norm(x[1],t_allium)\r\n    x3=norm(x[2],t_orchid)\r\n    x4=norm(x[3],t_tulip)\r\n    lis=[x1,x2,x3,x4]\r\n    return lis\r\n# because each sample is the list that contain 4 attributes\r\n#so we should compute the normalized value separately\r\n\r\nnt_daisy=normlist(t_daisy)\r\nnt_allium=normlist(t_allium)\r\nnt_orchid=normlist(t_orchid)\r\nnt_tulip=normlist(t_tulip)\r\n\r\ntrain_dataset=list(zip(nt_daisy,nt_allium,nt_orchid,nt_tulip))\r\n#zip 4 variables as a list\r\n  \r\n\r\n# Top K neighbors\r\ndef KNN(sample,t_dataset,k,t_label):\r\n    count=[]\r\n    #normsample=normsample(sample)\r\n    for x in range(len(t_dataset)):\r\n        count.append([Euclidean_dis(normsample(sample),t_dataset[x]),t_label[x]])\r\n        \r\n    count.sort() #sort the list and take top k values\r\n    kn=count[:k]\r\n    predict=0\r\n    for x in kn:\r\n        predict+=x[1]\r\n\r\n    if predict/k>0.5:\r\n        return 1\r\n    else:\r\n        return 0\r\n\t#determine the properties of the sample according to the \r\n\t#properties of the neighbors.\r\n\t\t\r\n\t\t\r\n#print(KNN([3,6,1,9],train_dataset,3,t_label))",
        "answer": "",
        "plotname": "",
        "haveplot": false,
        "nextLocked": false
    }
]